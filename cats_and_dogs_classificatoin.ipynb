{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_and_dogs_classificatoin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gowtham91m/cats-and-dogs-classification/blob/master/cats_and_dogs_classificatoin.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "iVLV8qY1LJNC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import io, cv2, fnmatch, shutil, os, getpass, subprocess, random\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "from time import time\n",
        "from glob import glob\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrEiqvOKuhuO",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ac01ee71-5d92-416d-8ce2-c0b6c4700930"
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "if 'kaggle.txt' not in os.listdir('/content'):\n",
        "  from google.colab import files\n",
        "  downloaded = files.upload()\n",
        "  \n",
        "os.chdir('/content')\n",
        "with open('kaggle.txt') as f: key = f.read()\n",
        "os.environ['KAGGLE_USERNAME']=\"gowham91m\"\n",
        "os.environ['KAGGLE_KEY']=key\n",
        "if 'cats_dogs' in os.listdir('/content'):shutil.rmtree('/content/cats_dogs')\n",
        "os.mkdir('/content/cats_dogs')\n",
        "os.chdir('/content/cats_dogs')\n",
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a216ba76-1c74-4f50-a17a-479c5b641f7a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a216ba76-1c74-4f50-a17a-479c5b641f7a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.txt to kaggle.txt\n",
            "Downloading sampleSubmission.csv to /content/cats_dogs\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 43.6MB/s]\n",
            "Downloading test1.zip to /content/cats_dogs\n",
            " 92% 250M/271M [00:03<00:00, 51.6MB/s]\n",
            "100% 271M/271M [00:03<00:00, 75.6MB/s]\n",
            "Downloading train.zip to /content/cats_dogs\n",
            " 99% 537M/543M [00:08<00:00, 62.5MB/s]\n",
            "100% 543M/543M [00:08<00:00, 69.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3wrOx42FijD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "32e777ce-e169-49ab-87d5-c62b33596bd1"
      },
      "cell_type": "code",
      "source": [
        "!unzip -q -o train.zip\n",
        "!unzip -q -o test1.zip\n",
        "\n",
        "cat_pattern = '*cat.*.jpg'\n",
        "dog_pattern = '*dog.*.jpg'\n",
        "\n",
        "images = glob('/content/cats_dogs/train/*.jpg', recursive=True)\n",
        "cats = fnmatch.filter(images,cat_pattern)\n",
        "dogs = fnmatch.filter(images,dog_pattern)\n",
        "\n",
        "os.listdir('/content/cats_dogs')\n",
        "if 'data' not in os.listdir('/content/cats_dogs'):os.mkdir('/content/cats_dogs/data')\n",
        "if 'train' not in os.listdir('/content/cats_dogs/data'):os.mkdir('/content/cats_dogs/data/train')\n",
        "if 'dogs' not in os.listdir('/content/cats_dogs/data/train'):os.mkdir('/content/cats_dogs/data/train/dogs')\n",
        "if 'cats' not in os.listdir('/content/cats_dogs/data/train'):os.mkdir('/content/cats_dogs/data/train/cats')\n",
        "  \n",
        "if 'val' not in os.listdir('/content/cats_dogs/data'):os.mkdir('/content/cats_dogs/data/val')\n",
        "if 'dogs' not in os.listdir('/content/cats_dogs/data/val'):os.mkdir('/content/cats_dogs/data/val/dogs')\n",
        "if 'cats' not in os.listdir('/content/cats_dogs/data/val'):os.mkdir('/content/cats_dogs/data/val/cats')\n",
        "\n",
        "train_dogs_path = '/content/cats_dogs/data/train/dogs'\n",
        "train_cats_path = '/content/cats_dogs/data/train/cats'\n",
        "\n",
        "val_dogs_path = '/content/cats_dogs/data/val/dogs'\n",
        "val_cats_path = '/content/cats_dogs/data/val/cats'\n",
        "\n",
        "for file in cats: shutil.copy2(file, train_cats_path)\n",
        "for file in dogs: shutil.copy2(file, train_dogs_path)\n",
        "  \n",
        "  \n",
        "# split train date into train and validation\n",
        "train_len = len(os.listdir('/content/cats_dogs/data/train/dogs'))\n",
        "val_len = train_len * 0.3\n",
        "val_dogs = random.sample(os.listdir(train_dogs_path),int(val_len))\n",
        "val_cats = random.sample(os.listdir(train_cats_path),int(val_len))\n",
        "\n",
        "\n",
        "for file in val_dogs:\n",
        "  try: shutil.move(os.path.join(train_dogs_path,file), val_dogs_path)\n",
        "  except: pass\n",
        "for file in val_cats:\n",
        "  try: shutil.move(os.path.join(train_cats_path,file), val_cats_path)\n",
        "  except: pass\n",
        "  \n",
        "print(len(os.listdir(train_cats_path)))\n",
        "print(len(os.listdir(val_cats_path)))\n",
        "\n",
        "print(len(os.listdir(train_dogs_path)))\n",
        "print(len(os.listdir(val_dogs_path)))\n",
        "\n",
        "print('total train samples ', len(os.listdir(train_cats_path)) + len(os.listdir(train_dogs_path)))\n",
        "print('total train samples ', len(os.listdir(val_cats_path)) + len(os.listdir(val_dogs_path)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8750\n",
            "3750\n",
            "8750\n",
            "3750\n",
            "total train samples  17500\n",
            "total train samples  7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPtRx1P_vCBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#CNN classifier"
      ]
    },
    {
      "metadata": {
        "id": "M8bbF3egoK2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1114
        },
        "outputId": "54457061-ec3d-4368-d424-799d57ecdb07"
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=32,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/32\n",
            "274/274 [==============================] - 188s 686ms/step - loss: 0.6740 - acc: 0.5869 - val_loss: 0.6211 - val_acc: 0.6712\n",
            "Epoch 2/32\n",
            "274/274 [==============================] - 179s 655ms/step - loss: 0.5808 - acc: 0.6965 - val_loss: 0.5099 - val_acc: 0.7443\n",
            "Epoch 3/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.5189 - acc: 0.7459 - val_loss: 0.4816 - val_acc: 0.7660\n",
            "Epoch 4/32\n",
            "274/274 [==============================] - 179s 655ms/step - loss: 0.4636 - acc: 0.7802 - val_loss: 0.4559 - val_acc: 0.7885\n",
            "Epoch 5/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.4238 - acc: 0.8046 - val_loss: 0.4466 - val_acc: 0.8041\n",
            "Epoch 6/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.3948 - acc: 0.8210 - val_loss: 0.3794 - val_acc: 0.8220\n",
            "Epoch 7/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.3596 - acc: 0.8415 - val_loss: 0.3292 - val_acc: 0.8616\n",
            "Epoch 8/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.3408 - acc: 0.8513 - val_loss: 0.3552 - val_acc: 0.8372\n",
            "Epoch 9/32\n",
            "274/274 [==============================] - 179s 653ms/step - loss: 0.3241 - acc: 0.8608 - val_loss: 0.3533 - val_acc: 0.8445\n",
            "Epoch 10/32\n",
            "274/274 [==============================] - 179s 652ms/step - loss: 0.3010 - acc: 0.8737 - val_loss: 0.3429 - val_acc: 0.8444\n",
            "Epoch 11/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.2818 - acc: 0.8802 - val_loss: 0.2596 - val_acc: 0.8911\n",
            "Epoch 12/32\n",
            "274/274 [==============================] - 180s 655ms/step - loss: 0.2660 - acc: 0.8868 - val_loss: 0.2561 - val_acc: 0.8924\n",
            "Epoch 13/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.2605 - acc: 0.8879 - val_loss: 0.2719 - val_acc: 0.8875\n",
            "Epoch 14/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.2396 - acc: 0.9006 - val_loss: 0.2544 - val_acc: 0.8993\n",
            "Epoch 15/32\n",
            "274/274 [==============================] - 180s 658ms/step - loss: 0.2326 - acc: 0.9043 - val_loss: 0.2647 - val_acc: 0.8960\n",
            "Epoch 16/32\n",
            "274/274 [==============================] - 181s 659ms/step - loss: 0.2189 - acc: 0.9107 - val_loss: 0.2166 - val_acc: 0.9083\n",
            "Epoch 17/32\n",
            "274/274 [==============================] - 179s 655ms/step - loss: 0.2139 - acc: 0.9140 - val_loss: 0.2131 - val_acc: 0.9143\n",
            "Epoch 18/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.2009 - acc: 0.9161 - val_loss: 0.2629 - val_acc: 0.8951\n",
            "Epoch 19/32\n",
            "274/274 [==============================] - 179s 653ms/step - loss: 0.2041 - acc: 0.9194 - val_loss: 0.4266 - val_acc: 0.8323\n",
            "Epoch 20/32\n",
            "274/274 [==============================] - 180s 655ms/step - loss: 0.1886 - acc: 0.9233 - val_loss: 0.2267 - val_acc: 0.9103\n",
            "Epoch 21/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.1995 - acc: 0.9206 - val_loss: 0.2325 - val_acc: 0.9051\n",
            "Epoch 22/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.1818 - acc: 0.9286 - val_loss: 0.2306 - val_acc: 0.9137\n",
            "Epoch 23/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.1849 - acc: 0.9283 - val_loss: 0.2134 - val_acc: 0.9184\n",
            "Epoch 24/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.1790 - acc: 0.9291 - val_loss: 0.2116 - val_acc: 0.9252\n",
            "Epoch 25/32\n",
            "274/274 [==============================] - 179s 652ms/step - loss: 0.1838 - acc: 0.9294 - val_loss: 0.2344 - val_acc: 0.9049\n",
            "Epoch 26/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.1804 - acc: 0.9310 - val_loss: 0.2289 - val_acc: 0.9165\n",
            "Epoch 27/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.1714 - acc: 0.9315 - val_loss: 0.2055 - val_acc: 0.9235\n",
            "Epoch 28/32\n",
            "274/274 [==============================] - 180s 658ms/step - loss: 0.1740 - acc: 0.9317 - val_loss: 0.2156 - val_acc: 0.9120\n",
            "Epoch 29/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.1733 - acc: 0.9314 - val_loss: 0.2190 - val_acc: 0.9237\n",
            "Epoch 30/32\n",
            "274/274 [==============================] - 180s 658ms/step - loss: 0.1724 - acc: 0.9307 - val_loss: 0.2098 - val_acc: 0.9240\n",
            "Epoch 31/32\n",
            "274/274 [==============================] - 180s 655ms/step - loss: 0.1706 - acc: 0.9340 - val_loss: 0.2361 - val_acc: 0.9079\n",
            "Epoch 32/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.1666 - acc: 0.9358 - val_loss: 0.2564 - val_acc: 0.9175\n",
            "time taken  5755.778434276581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V9zA0ChBu75g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Transfer learning"
      ]
    },
    {
      "metadata": {
        "id": "foaln3KF94Nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "dd03be89-c235-4a66-9c3a-d8dd7e08fe2d"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'VGG16'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "vgg_model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    \n",
        "vgg_model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "vgg_model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "vgg_model.save_weights('VGG16.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "274/274 [==============================] - 326s 1s/step - loss: 0.4903 - acc: 0.7797 - val_loss: 0.3391 - val_acc: 0.8727\n",
            "Epoch 2/16\n",
            "274/274 [==============================] - 311s 1s/step - loss: 0.3377 - acc: 0.8726 - val_loss: 0.2705 - val_acc: 0.8969\n",
            "Epoch 3/16\n",
            "274/274 [==============================] - 313s 1s/step - loss: 0.2928 - acc: 0.8849 - val_loss: 0.2416 - val_acc: 0.9075\n",
            "Epoch 4/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2664 - acc: 0.8961 - val_loss: 0.2263 - val_acc: 0.9132\n",
            "Epoch 5/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.2507 - acc: 0.9002 - val_loss: 0.2202 - val_acc: 0.9147\n",
            "Epoch 6/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2404 - acc: 0.9034 - val_loss: 0.1999 - val_acc: 0.9220\n",
            "Epoch 7/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.2319 - acc: 0.9062 - val_loss: 0.1922 - val_acc: 0.9260\n",
            "Epoch 8/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2230 - acc: 0.9110 - val_loss: 0.1868 - val_acc: 0.9277\n",
            "Epoch 9/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2168 - acc: 0.9130 - val_loss: 0.1811 - val_acc: 0.9307\n",
            "Epoch 10/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2119 - acc: 0.9163 - val_loss: 0.1813 - val_acc: 0.9267\n",
            "Epoch 11/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.2055 - acc: 0.9170 - val_loss: 0.1747 - val_acc: 0.9315\n",
            "Epoch 12/16\n",
            "274/274 [==============================] - 312s 1s/step - loss: 0.2009 - acc: 0.9176 - val_loss: 0.1700 - val_acc: 0.9321\n",
            "Epoch 13/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.1965 - acc: 0.9216 - val_loss: 0.1736 - val_acc: 0.9304\n",
            "Epoch 14/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.1942 - acc: 0.9214 - val_loss: 0.1692 - val_acc: 0.9324\n",
            "Epoch 15/16\n",
            "274/274 [==============================] - 313s 1s/step - loss: 0.1901 - acc: 0.9235 - val_loss: 0.1628 - val_acc: 0.9341\n",
            "Epoch 16/16\n",
            "274/274 [==============================] - 313s 1s/step - loss: 0.1866 - acc: 0.9267 - val_loss: 0.1607 - val_acc: 0.9364\n",
            "time taken  5039.052234888077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sb5kUTPTd7BJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "ba9b78fb-b19d-4616-f681-85e9fcace855"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'vgg19'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('vgg19-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('vgg19.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 370s 676ms/step - loss: 0.4825 - acc: 0.7809 - val_loss: 0.3768 - val_acc: 0.8321\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 355s 650ms/step - loss: 0.3353 - acc: 0.8632 - val_loss: 0.2976 - val_acc: 0.8768\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 358s 655ms/step - loss: 0.2954 - acc: 0.8792 - val_loss: 0.2573 - val_acc: 0.8976\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 357s 653ms/step - loss: 0.2747 - acc: 0.8856 - val_loss: 0.2554 - val_acc: 0.8881\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 355s 648ms/step - loss: 0.2577 - acc: 0.8926 - val_loss: 0.2327 - val_acc: 0.9069\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 355s 648ms/step - loss: 0.2473 - acc: 0.8987 - val_loss: 0.2223 - val_acc: 0.9065\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 356s 651ms/step - loss: 0.2362 - acc: 0.9054 - val_loss: 0.2143 - val_acc: 0.9140\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 348s 636ms/step - loss: 0.2318 - acc: 0.9040 - val_loss: 0.2114 - val_acc: 0.9112\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 347s 635ms/step - loss: 0.2274 - acc: 0.9040 - val_loss: 0.2051 - val_acc: 0.9167\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 346s 632ms/step - loss: 0.2218 - acc: 0.9076 - val_loss: 0.1972 - val_acc: 0.9189\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 344s 629ms/step - loss: 0.2142 - acc: 0.9103 - val_loss: 0.1991 - val_acc: 0.9171\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 347s 634ms/step - loss: 0.2124 - acc: 0.9119 - val_loss: 0.1901 - val_acc: 0.9227\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 351s 642ms/step - loss: 0.2083 - acc: 0.9156 - val_loss: 0.1873 - val_acc: 0.9232\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 352s 644ms/step - loss: 0.2075 - acc: 0.9146 - val_loss: 0.1894 - val_acc: 0.9212\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 349s 638ms/step - loss: 0.2043 - acc: 0.9164 - val_loss: 0.1829 - val_acc: 0.9248\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 352s 644ms/step - loss: 0.1966 - acc: 0.9188 - val_loss: 0.1854 - val_acc: 0.9224\n",
            "time taken  5653.399586439133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IlpiMhvMd69-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "a7682366-8ddd-4f5b-d300-ef7561639bc4"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'ResNet50'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('ResNet50-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('ResNet50.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 320s 585ms/step - loss: 0.3268 - acc: 0.8855 - val_loss: 0.1813 - val_acc: 0.9453\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 317s 580ms/step - loss: 0.1606 - acc: 0.9486 - val_loss: 0.1292 - val_acc: 0.9579\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 316s 577ms/step - loss: 0.1262 - acc: 0.9563 - val_loss: 0.1124 - val_acc: 0.9603\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 323s 590ms/step - loss: 0.1092 - acc: 0.9616 - val_loss: 0.0993 - val_acc: 0.9648\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 316s 577ms/step - loss: 0.1042 - acc: 0.9617 - val_loss: 0.0907 - val_acc: 0.9653\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.0989 - acc: 0.9637 - val_loss: 0.0899 - val_acc: 0.9669\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 317s 579ms/step - loss: 0.0946 - acc: 0.9646 - val_loss: 0.0845 - val_acc: 0.9685\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 323s 591ms/step - loss: 0.0875 - acc: 0.9690 - val_loss: 0.0872 - val_acc: 0.9667\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 321s 586ms/step - loss: 0.0924 - acc: 0.9657 - val_loss: 0.0807 - val_acc: 0.9693\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 325s 594ms/step - loss: 0.0845 - acc: 0.9692 - val_loss: 0.0816 - val_acc: 0.9680\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 319s 584ms/step - loss: 0.0834 - acc: 0.9678 - val_loss: 0.0804 - val_acc: 0.9685\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 325s 593ms/step - loss: 0.0832 - acc: 0.9681 - val_loss: 0.0756 - val_acc: 0.9697\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 319s 583ms/step - loss: 0.0791 - acc: 0.9711 - val_loss: 0.0772 - val_acc: 0.9689\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 326s 597ms/step - loss: 0.0803 - acc: 0.9695 - val_loss: 0.0722 - val_acc: 0.9721\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 318s 581ms/step - loss: 0.0770 - acc: 0.9705 - val_loss: 0.0764 - val_acc: 0.9699\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 323s 591ms/step - loss: 0.0766 - acc: 0.9715 - val_loss: 0.0727 - val_acc: 0.9709\n",
            "time taken  5178.096745729446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QiBcM20Cd67Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "1cb36ce5-e19b-424d-b842-0a3bc194d049"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'InceptionV3'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('InceptionV3-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('InceptionV3.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 3s 0us/step\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 290s 530ms/step - loss: 0.3235 - acc: 0.8516 - val_loss: 0.2213 - val_acc: 0.9057\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 281s 513ms/step - loss: 0.2277 - acc: 0.9031 - val_loss: 0.2134 - val_acc: 0.9081\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 284s 519ms/step - loss: 0.2046 - acc: 0.9133 - val_loss: 0.2145 - val_acc: 0.9087\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 282s 516ms/step - loss: 0.1987 - acc: 0.9170 - val_loss: 0.1949 - val_acc: 0.9192\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 281s 514ms/step - loss: 0.1874 - acc: 0.9198 - val_loss: 0.1916 - val_acc: 0.9215\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 281s 513ms/step - loss: 0.1837 - acc: 0.9210 - val_loss: 0.1814 - val_acc: 0.9247\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 285s 520ms/step - loss: 0.1737 - acc: 0.9277 - val_loss: 0.1805 - val_acc: 0.9256\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 282s 515ms/step - loss: 0.1779 - acc: 0.9257 - val_loss: 0.1945 - val_acc: 0.9219\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 283s 517ms/step - loss: 0.1728 - acc: 0.9268 - val_loss: 0.1852 - val_acc: 0.9228\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 285s 520ms/step - loss: 0.1665 - acc: 0.9300 - val_loss: 0.1798 - val_acc: 0.9240\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 283s 517ms/step - loss: 0.1645 - acc: 0.9317 - val_loss: 0.1806 - val_acc: 0.9253\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 282s 516ms/step - loss: 0.1690 - acc: 0.9290 - val_loss: 0.1769 - val_acc: 0.9241\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 283s 517ms/step - loss: 0.1656 - acc: 0.9298 - val_loss: 0.1727 - val_acc: 0.9268\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 283s 517ms/step - loss: 0.1509 - acc: 0.9366 - val_loss: 0.1723 - val_acc: 0.9245\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 281s 515ms/step - loss: 0.1543 - acc: 0.9362 - val_loss: 0.1719 - val_acc: 0.9255\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 281s 514ms/step - loss: 0.1570 - acc: 0.9346 - val_loss: 0.1817 - val_acc: 0.9232\n",
            "time taken  4633.057312965393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o22GjFjPnIM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "bf9b41b9-d8ab-464e-fa14-a6822fa0a294"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'Xception'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 2s 0us/step\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 352s 643ms/step - loss: 0.1512 - acc: 0.9373 - val_loss: 0.0958 - val_acc: 0.9623\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 341s 624ms/step - loss: 0.1019 - acc: 0.9588 - val_loss: 0.0990 - val_acc: 0.9604\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 342s 624ms/step - loss: 0.0968 - acc: 0.9617 - val_loss: 0.0896 - val_acc: 0.9628\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 340s 622ms/step - loss: 0.0875 - acc: 0.9656 - val_loss: 0.0835 - val_acc: 0.9672\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 341s 623ms/step - loss: 0.0867 - acc: 0.9656 - val_loss: 0.0862 - val_acc: 0.9651\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 343s 627ms/step - loss: 0.0861 - acc: 0.9660 - val_loss: 0.0841 - val_acc: 0.9669\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 342s 626ms/step - loss: 0.0787 - acc: 0.9697 - val_loss: 0.0919 - val_acc: 0.9651\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 341s 624ms/step - loss: 0.0822 - acc: 0.9678 - val_loss: 0.0843 - val_acc: 0.9673\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 339s 621ms/step - loss: 0.0814 - acc: 0.9682 - val_loss: 0.0873 - val_acc: 0.9660\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 341s 624ms/step - loss: 0.0746 - acc: 0.9713 - val_loss: 0.0836 - val_acc: 0.9672\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 342s 624ms/step - loss: 0.0758 - acc: 0.9698 - val_loss: 0.0810 - val_acc: 0.9696\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 340s 621ms/step - loss: 0.0756 - acc: 0.9704 - val_loss: 0.0827 - val_acc: 0.9664\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 341s 623ms/step - loss: 0.0708 - acc: 0.9722 - val_loss: 0.0840 - val_acc: 0.9657\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 341s 623ms/step - loss: 0.0731 - acc: 0.9716 - val_loss: 0.0859 - val_acc: 0.9656\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 342s 625ms/step - loss: 0.0735 - acc: 0.9714 - val_loss: 0.0855 - val_acc: 0.9677\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 342s 626ms/step - loss: 0.0709 - acc: 0.9731 - val_loss: 0.0801 - val_acc: 0.9689\n",
            "time taken  5525.559000015259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f4IBmwsj9IMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "9cd8aa50-5f57-4377-df97-2a9d019fc49f"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'DenseNet169'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(250, 250),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(250, 250),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 250, 250, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "53182464/53178568 [==============================] - 1s 0us/step\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 476s 870ms/step - loss: 0.0731 - acc: 0.9710 - val_loss: 0.0445 - val_acc: 0.9824\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 456s 834ms/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0483 - val_acc: 0.9823\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 458s 837ms/step - loss: 0.0398 - acc: 0.9850 - val_loss: 0.0430 - val_acc: 0.9836\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 460s 840ms/step - loss: 0.0308 - acc: 0.9883 - val_loss: 0.0415 - val_acc: 0.9836\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 456s 833ms/step - loss: 0.0275 - acc: 0.9900 - val_loss: 0.0396 - val_acc: 0.9845\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 456s 833ms/step - loss: 0.0230 - acc: 0.9909 - val_loss: 0.0417 - val_acc: 0.9844\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 456s 834ms/step - loss: 0.0206 - acc: 0.9922 - val_loss: 0.0386 - val_acc: 0.9864\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 457s 836ms/step - loss: 0.0192 - acc: 0.9928 - val_loss: 0.0364 - val_acc: 0.9851\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 455s 832ms/step - loss: 0.0206 - acc: 0.9929 - val_loss: 0.0439 - val_acc: 0.9831\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 457s 836ms/step - loss: 0.0164 - acc: 0.9945 - val_loss: 0.0384 - val_acc: 0.9864\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 456s 834ms/step - loss: 0.0159 - acc: 0.9940 - val_loss: 0.0372 - val_acc: 0.9857\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 458s 838ms/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0388 - val_acc: 0.9859\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 458s 837ms/step - loss: 0.0147 - acc: 0.9946 - val_loss: 0.0389 - val_acc: 0.9853\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 459s 839ms/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0368 - val_acc: 0.9868\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 459s 839ms/step - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0388 - val_acc: 0.9867\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 457s 835ms/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0339 - val_acc: 0.9872\n",
            "time taken  7625.3587481975555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WUMb3B7X_w7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "7cce1b27-de0a-41a7-af2a-e727b7a9509c"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'DenseNet121'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(250, 250),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(250, 250),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 250, 250, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "30015488/30011760 [==============================] - 3s 0us/step\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 421s 770ms/step - loss: 0.0930 - acc: 0.9625 - val_loss: 0.0699 - val_acc: 0.9745\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 407s 743ms/step - loss: 0.0505 - acc: 0.9814 - val_loss: 0.0599 - val_acc: 0.9783\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 410s 750ms/step - loss: 0.0420 - acc: 0.9834 - val_loss: 0.0575 - val_acc: 0.9800\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 409s 748ms/step - loss: 0.0380 - acc: 0.9854 - val_loss: 0.0542 - val_acc: 0.9787\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 421s 770ms/step - loss: 0.0327 - acc: 0.9870 - val_loss: 0.0583 - val_acc: 0.9781\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 421s 769ms/step - loss: 0.0322 - acc: 0.9870 - val_loss: 0.0514 - val_acc: 0.9817\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 423s 773ms/step - loss: 0.0279 - acc: 0.9901 - val_loss: 0.0542 - val_acc: 0.9803\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 424s 775ms/step - loss: 0.0256 - acc: 0.9906 - val_loss: 0.0519 - val_acc: 0.9820\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 425s 777ms/step - loss: 0.0276 - acc: 0.9898 - val_loss: 0.0515 - val_acc: 0.9811\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 424s 774ms/step - loss: 0.0251 - acc: 0.9915 - val_loss: 0.0493 - val_acc: 0.9804\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 423s 773ms/step - loss: 0.0230 - acc: 0.9918 - val_loss: 0.0563 - val_acc: 0.9793\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 423s 774ms/step - loss: 0.0197 - acc: 0.9931 - val_loss: 0.0504 - val_acc: 0.9827\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 422s 771ms/step - loss: 0.0219 - acc: 0.9925 - val_loss: 0.0542 - val_acc: 0.9819\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 421s 770ms/step - loss: 0.0191 - acc: 0.9934 - val_loss: 0.0549 - val_acc: 0.9801\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 423s 773ms/step - loss: 0.0176 - acc: 0.9939 - val_loss: 0.0544 - val_acc: 0.9815\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 421s 770ms/step - loss: 0.0173 - acc: 0.9937 - val_loss: 0.0521 - val_acc: 0.9811\n",
            "time taken  6748.729496717453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lol2dij_9Kit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# running on TPU\n",
        "\n",
        "# !pip install keras==2.1\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "# from tensorflow.keras.models import Sequential, Model\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "\n",
        "BASE_MODEL = 'Xception'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model.compile(loss='binary_crossentropy'\n",
        "              ,optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator\n",
        "        #class_weight = class_weights,\n",
        "        ,callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)]\n",
        "        )\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}