{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_and_dogs_classificatoin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gowtham91m/cats-and-dogs-classification/blob/master/cats_and_dogs_classificatoin.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "iVLV8qY1LJNC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import io, cv2, fnmatch, shutil, os, getpass, subprocess, random\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from glob import glob\n",
        "from sklearn.utils import class_weight\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JlWMCj3CakyP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_dir = '/content'\n",
        "download_dir = os.path.join(root_dir,'cats_dogs')\n",
        "data_dir = os.path.join(download_dir,'data')\n",
        "train_path = os.path.join(data_dir,'train')\n",
        "val_path = os.path.join(data_dir,'val')\n",
        "train_dogs_path = os.path.join(train_path,'dogs')\n",
        "train_cats_path = os.path.join(train_path,'cats')\n",
        "val_dogs_path = os.path.join(val_path,'dogs')\n",
        "val_cats_path = os.path.join(val_path,'cats')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSfAxmPpa6qG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "data source - https://www.kaggle.com/c/dogs-vs-cats/data"
      ]
    },
    {
      "metadata": {
        "id": "lekG8HpxV3Aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e099c392-f0c4-4001-e15a-726edc7f0364"
      },
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = root_dir\n",
        "os.chdir(root_dir)\n",
        "if 'kaggle.json' not in os.listdir(root_dir):downloaded = files.upload()\n",
        "if 'cats_dogs' in os.listdir(root_dir):shutil.rmtree(download_dir)\n",
        "os.mkdir(download_dir)\n",
        "os.chdir(download_dir)\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -q -o train.zip\n",
        "!unzip -q -o test1.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /content/kaggle.json'\n",
            "Downloading sampleSubmission.csv to /content/cats_dogs\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 38.3MB/s]\n",
            "Downloading test1.zip to /content/cats_dogs\n",
            " 98% 267M/271M [00:03<00:00, 116MB/s] \n",
            "100% 271M/271M [00:03<00:00, 91.0MB/s]\n",
            "Downloading train.zip to /content/cats_dogs\n",
            " 96% 521M/543M [00:07<00:00, 92.7MB/s]\n",
            "100% 543M/543M [00:07<00:00, 72.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3wrOx42FijD",
        "colab_type": "code",
        "outputId": "14998b9d-3b86-463c-e75b-6b0b8c6bea97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "cat_pattern = '*cat.*.jpg'\n",
        "dog_pattern = '*dog.*.jpg'\n",
        "\n",
        "images = glob('/content/cats_dogs/train/*.jpg', recursive=True)\n",
        "cats = fnmatch.filter(images,cat_pattern)\n",
        "dogs = fnmatch.filter(images,dog_pattern)\n",
        "\n",
        "if 'data' not in os.listdir(download_dir):os.mkdir(data_dir)\n",
        "if 'train' not in os.listdir(data_dir):os.mkdir(train_path)\n",
        "if 'dogs' not in os.listdir(train_path):os.mkdir(train_dogs_path)\n",
        "if 'cats' not in os.listdir(train_path):os.mkdir(train_cats_path)\n",
        "  \n",
        "if 'val' not in os.listdir(data_dir):os.mkdir(val_path)\n",
        "if 'dogs' not in os.listdir(val_path):os.mkdir(val_dogs_path)\n",
        "if 'cats' not in os.listdir(val_path):os.mkdir(val_cats_path)\n",
        "\n",
        "for file in cats: shutil.copy2(file, train_cats_path)\n",
        "for file in dogs: shutil.copy2(file, train_dogs_path)\n",
        "  \n",
        "  \n",
        "# split train date into train and validation\n",
        "train_len = len(os.listdir(train_dogs_path))\n",
        "val_len = train_len * 0.3\n",
        "val_dogs = random.sample(os.listdir(train_dogs_path),int(val_len))\n",
        "val_cats = random.sample(os.listdir(train_cats_path),int(val_len))\n",
        "\n",
        "for file in val_dogs:\n",
        "  try: shutil.move(os.path.join(train_dogs_path,file), val_dogs_path)\n",
        "  except: pass\n",
        "for file in val_cats:\n",
        "  try: shutil.move(os.path.join(train_cats_path,file), val_cats_path)\n",
        "  except: pass\n",
        "  \n",
        "print(len(os.listdir(train_cats_path)))\n",
        "print(len(os.listdir(val_cats_path)))\n",
        "\n",
        "print(len(os.listdir(train_dogs_path)))\n",
        "print(len(os.listdir(val_dogs_path)))\n",
        "\n",
        "print('total train samples ', len(os.listdir(train_cats_path)) + len(os.listdir(train_dogs_path)))\n",
        "print('total train samples ', len(os.listdir(val_cats_path)) + len(os.listdir(val_dogs_path)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8750\n",
            "3750\n",
            "8750\n",
            "3750\n",
            "total train samples  17500\n",
            "total train samples  7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPtRx1P_vCBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#CNN classifier"
      ]
    },
    {
      "metadata": {
        "id": "M8bbF3egoK2g",
        "colab_type": "code",
        "outputId": "f74e8536-490f-4a36-98b3-00e3073a4df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Epoch 1/16\n",
            "274/274 [==============================] - 185s 674ms/step - loss: 0.6532 - acc: 0.6105 - val_loss: 0.5945 - val_acc: 0.6825\n",
            "Epoch 2/16\n",
            "274/274 [==============================] - 176s 644ms/step - loss: 0.5644 - acc: 0.7116 - val_loss: 0.4892 - val_acc: 0.7596\n",
            "Epoch 3/16\n",
            "274/274 [==============================] - 175s 639ms/step - loss: 0.5034 - acc: 0.7541 - val_loss: 0.5124 - val_acc: 0.7473\n",
            "Epoch 4/16\n",
            "274/274 [==============================] - 177s 644ms/step - loss: 0.4489 - acc: 0.7907 - val_loss: 0.3812 - val_acc: 0.8313\n",
            "Epoch 5/16\n",
            "274/274 [==============================] - 176s 642ms/step - loss: 0.4105 - acc: 0.8101 - val_loss: 0.3582 - val_acc: 0.8409\n",
            "Epoch 6/16\n",
            "274/274 [==============================] - 177s 647ms/step - loss: 0.3770 - acc: 0.8320 - val_loss: 0.3582 - val_acc: 0.8429\n",
            "Epoch 7/16\n",
            "274/274 [==============================] - 177s 647ms/step - loss: 0.3444 - acc: 0.8483 - val_loss: 0.3192 - val_acc: 0.8564\n",
            "Epoch 8/16\n",
            "274/274 [==============================] - 176s 642ms/step - loss: 0.3144 - acc: 0.8650 - val_loss: 0.4836 - val_acc: 0.7736\n",
            "Epoch 9/16\n",
            "274/274 [==============================] - 176s 642ms/step - loss: 0.2939 - acc: 0.8710 - val_loss: 0.2954 - val_acc: 0.8697\n",
            "Epoch 10/16\n",
            "274/274 [==============================] - 176s 641ms/step - loss: 0.2726 - acc: 0.8846 - val_loss: 0.3313 - val_acc: 0.8673\n",
            "Epoch 11/16\n",
            "274/274 [==============================] - 176s 644ms/step - loss: 0.2574 - acc: 0.8918 - val_loss: 0.3142 - val_acc: 0.8720\n",
            "Epoch 12/16\n",
            "274/274 [==============================] - 176s 642ms/step - loss: 0.2426 - acc: 0.8986 - val_loss: 0.2834 - val_acc: 0.8837\n",
            "Epoch 13/16\n",
            "274/274 [==============================] - 177s 645ms/step - loss: 0.2284 - acc: 0.9060 - val_loss: 0.2298 - val_acc: 0.9047\n",
            "Epoch 14/16\n",
            "274/274 [==============================] - 177s 646ms/step - loss: 0.2155 - acc: 0.9103 - val_loss: 0.2316 - val_acc: 0.9055\n",
            "Epoch 15/16\n",
            "274/274 [==============================] - 176s 643ms/step - loss: 0.2104 - acc: 0.9137 - val_loss: 0.2269 - val_acc: 0.9033\n",
            "Epoch 16/16\n",
            "274/274 [==============================] - 176s 642ms/step - loss: 0.1997 - acc: 0.9186 - val_loss: 0.2835 - val_acc: 0.8928\n",
            "time taken  2829.037061691284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qx21VUQAQz4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V9zA0ChBu75g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Transfer learning"
      ]
    },
    {
      "metadata": {
        "id": "oms7_s2lvVVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#BASE_MODEL = 'VGG16'\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def transfer_learning(BASE_MODEL):\n",
        "  if BASE_MODEL=='VGG16':\n",
        "      from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='vgg19':\n",
        "      from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='ResNet50':\n",
        "      from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='InceptionV3':\n",
        "      from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='Xception':\n",
        "      from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet169': \n",
        "      from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet121':\n",
        "      from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "  else:\n",
        "      raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "  \n",
        "  import keras\n",
        "  keras.backend.set_learning_phase(1)\n",
        "  \n",
        "  check_point_name = BASE_MODEL + '.model'\n",
        "  model_weights = BASE_MODEL + '.h5'\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "          rescale=1./255,\n",
        "          shear_range=0.2,\n",
        "          zoom_range=0.2,\n",
        "          horizontal_flip=True,\n",
        "          preprocessing_function = preprocess_input)\n",
        "\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  preprocessing_function = preprocess_input)\n",
        "\n",
        "  batch_size=32\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          '/content/cats_dogs/data/train',  # this is the target directory\n",
        "          target_size=(250, 250),  # all images will be resized to 150x150\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary') \n",
        "\n",
        "  validation_generator = val_datagen.flow_from_directory(\n",
        "          '/content/cats_dogs/data/val',\n",
        "          target_size=(250, 250),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary')    \n",
        "\n",
        "  img_rows, img_cols, img_channel = 250, 250, 3\n",
        "  base_model = PTModel(weights='imagenet'\n",
        "                     ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "  add_model = Sequential()\n",
        "  add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "  add_model.add(Dense(64, activation='relu'))\n",
        "  add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "      if layer.name.startswith('bn'):\n",
        "          layer.call(layer.input, training=False)\n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "                metrics=['accuracy'])\n",
        "  start_time = time()\n",
        "  model.fit_generator(\n",
        "          train_generator,\n",
        "          epochs=16,\n",
        "          validation_data=validation_generator,\n",
        "          #class_weight = class_weights,\n",
        "          callbacks=[ModelCheckpoint(check_point_name, monitor='val_acc', save_best_only=True)])\n",
        "  model.save_weights(model_weights)\n",
        "\n",
        "  print('time taken ',time()-start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kf1P8MRCvXeW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "90e140da-7ea3-4855-d60c-6544d079b20e"
      },
      "cell_type": "code",
      "source": [
        "transfer_learning('VGG16')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 330s 604ms/step - loss: 0.4231 - acc: 0.8201 - val_loss: 0.2916 - val_acc: 0.8900\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 322s 588ms/step - loss: 0.2869 - acc: 0.8872 - val_loss: 0.2411 - val_acc: 0.9068\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 322s 588ms/step - loss: 0.2473 - acc: 0.9021 - val_loss: 0.2187 - val_acc: 0.9136\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 321s 588ms/step - loss: 0.2302 - acc: 0.9078 - val_loss: 0.2067 - val_acc: 0.9151\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.2163 - acc: 0.9111 - val_loss: 0.2091 - val_acc: 0.9135\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 321s 586ms/step - loss: 0.2038 - acc: 0.9180 - val_loss: 0.1885 - val_acc: 0.9231\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1999 - acc: 0.9186 - val_loss: 0.2097 - val_acc: 0.9112\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1920 - acc: 0.9236 - val_loss: 0.1841 - val_acc: 0.9215\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1861 - acc: 0.9268 - val_loss: 0.1765 - val_acc: 0.9281\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1812 - acc: 0.9263 - val_loss: 0.1722 - val_acc: 0.9291\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1780 - acc: 0.9286 - val_loss: 0.1695 - val_acc: 0.9304\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1756 - acc: 0.9318 - val_loss: 0.1679 - val_acc: 0.9311\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1731 - acc: 0.9304 - val_loss: 0.1729 - val_acc: 0.9271\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1701 - acc: 0.9321 - val_loss: 0.1643 - val_acc: 0.9313\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1659 - acc: 0.9339 - val_loss: 0.1695 - val_acc: 0.9311\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.1631 - acc: 0.9337 - val_loss: 0.1870 - val_acc: 0.9197\n",
            "time taken  5149.907457113266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tOsIBkruvXig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "f2b7db6b-ba99-4aea-bf2c-abcf9f1ab3d4"
      },
      "cell_type": "code",
      "source": [
        "transfer_learning('DenseNet169')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 470s 860ms/step - loss: 0.0795 - acc: 0.9693 - val_loss: 0.0483 - val_acc: 0.9811\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 449s 821ms/step - loss: 0.0461 - acc: 0.9817 - val_loss: 0.0443 - val_acc: 0.9831\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 451s 825ms/step - loss: 0.0352 - acc: 0.9870 - val_loss: 0.0404 - val_acc: 0.9851\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 450s 823ms/step - loss: 0.0322 - acc: 0.9872 - val_loss: 0.0399 - val_acc: 0.9841\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 450s 822ms/step - loss: 0.0277 - acc: 0.9901 - val_loss: 0.0429 - val_acc: 0.9848\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 452s 826ms/step - loss: 0.0253 - acc: 0.9910 - val_loss: 0.0379 - val_acc: 0.9853\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 451s 824ms/step - loss: 0.0218 - acc: 0.9920 - val_loss: 0.0372 - val_acc: 0.9859\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 449s 821ms/step - loss: 0.0211 - acc: 0.9923 - val_loss: 0.0445 - val_acc: 0.9839\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 449s 821ms/step - loss: 0.0189 - acc: 0.9935 - val_loss: 0.0489 - val_acc: 0.9828\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 447s 818ms/step - loss: 0.0168 - acc: 0.9943 - val_loss: 0.0380 - val_acc: 0.9863\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 447s 817ms/step - loss: 0.0182 - acc: 0.9937 - val_loss: 0.0416 - val_acc: 0.9860\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 448s 819ms/step - loss: 0.0160 - acc: 0.9943 - val_loss: 0.0395 - val_acc: 0.9851\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 448s 819ms/step - loss: 0.0165 - acc: 0.9943 - val_loss: 0.0404 - val_acc: 0.9847\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 448s 819ms/step - loss: 0.0149 - acc: 0.9944 - val_loss: 0.0421 - val_acc: 0.9851\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 448s 820ms/step - loss: 0.0133 - acc: 0.9955 - val_loss: 0.0412 - val_acc: 0.9857\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 448s 818ms/step - loss: 0.0146 - acc: 0.9955 - val_loss: 0.0351 - val_acc: 0.9883\n",
            "time taken  7633.359741687775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lol2dij_9Kit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# running on TPU\n",
        "\n",
        "# !pip install keras==2.1\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "# from tensorflow.keras.models import Sequential, Model\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "\n",
        "BASE_MODEL = 'VGG16'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model.compile(loss='binary_crossentropy'\n",
        "              ,optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator\n",
        "        #class_weight = class_weights,\n",
        "        ,callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)]\n",
        "        )\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}