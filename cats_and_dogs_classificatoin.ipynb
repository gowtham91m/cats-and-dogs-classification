{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_and_dogs_classificatoin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gowtham91m/cats-and-dogs-classification/blob/master/cats_and_dogs_classificatoin.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "iVLV8qY1LJNC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import io, cv2, fnmatch, shutil, os, getpass, subprocess, random\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "from time import time\n",
        "from glob import glob\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrEiqvOKuhuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "e564976c-02e2-4447-fda4-06ff02768f40"
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "if 'kaggle.txt' not in os.listdir('/content'):\n",
        "  from google.colab import files\n",
        "  downloaded = files.upload()\n",
        "  \n",
        "os.chdir('/content')\n",
        "with open('kaggle.txt') as f: key = f.read()\n",
        "os.environ['KAGGLE_USERNAME']=\"gowham91m\"\n",
        "os.environ['KAGGLE_KEY']=key\n",
        "if 'cats_dogs' in os.listdir('/content'):shutil.rmtree('/content/cats_dogs')\n",
        "os.mkdir('/content/cats_dogs')\n",
        "os.chdir('/content/cats_dogs')\n",
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sampleSubmission.csv to /content/cats_dogs\n",
            "\r  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 36.8MB/s]\n",
            "Downloading test1.zip to /content/cats_dogs\n",
            " 94% 254M/271M [00:02<00:00, 123MB/s]\n",
            "100% 271M/271M [00:02<00:00, 132MB/s]\n",
            "Downloading train.zip to /content/cats_dogs\n",
            " 98% 534M/543M [00:03<00:00, 99.7MB/s]\n",
            "100% 543M/543M [00:03<00:00, 145MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3wrOx42FijD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "804b51f4-f6a0-4596-b7ee-114217493167"
      },
      "cell_type": "code",
      "source": [
        "!unzip -q -o train.zip\n",
        "!unzip -q -o test1.zip\n",
        "\n",
        "cat_pattern = '*cat.*.jpg'\n",
        "dog_pattern = '*dog.*.jpg'\n",
        "\n",
        "images = glob('/content/cats_dogs/train/*.jpg', recursive=True)\n",
        "cats = fnmatch.filter(images,cat_pattern)\n",
        "dogs = fnmatch.filter(images,dog_pattern)\n",
        "\n",
        "os.listdir('/content/cats_dogs')\n",
        "if 'data' not in os.listdir('/content/cats_dogs'):os.mkdir('/content/cats_dogs/data')\n",
        "if 'train' not in os.listdir('/content/cats_dogs/data'):os.mkdir('/content/cats_dogs/data/train')\n",
        "if 'dogs' not in os.listdir('/content/cats_dogs/data/train'):os.mkdir('/content/cats_dogs/data/train/dogs')\n",
        "if 'cats' not in os.listdir('/content/cats_dogs/data/train'):os.mkdir('/content/cats_dogs/data/train/cats')\n",
        "  \n",
        "if 'val' not in os.listdir('/content/cats_dogs/data'):os.mkdir('/content/cats_dogs/data/val')\n",
        "if 'dogs' not in os.listdir('/content/cats_dogs/data/val'):os.mkdir('/content/cats_dogs/data/val/dogs')\n",
        "if 'cats' not in os.listdir('/content/cats_dogs/data/val'):os.mkdir('/content/cats_dogs/data/val/cats')\n",
        "\n",
        "train_dogs_path = '/content/cats_dogs/data/train/dogs'\n",
        "train_cats_path = '/content/cats_dogs/data/train/cats'\n",
        "\n",
        "val_dogs_path = '/content/cats_dogs/data/val/dogs'\n",
        "val_cats_path = '/content/cats_dogs/data/val/cats'\n",
        "\n",
        "for file in cats: shutil.copy2(file, train_cats_path)\n",
        "for file in dogs: shutil.copy2(file, train_dogs_path)\n",
        "  \n",
        "  \n",
        "# split train date into train and validation\n",
        "train_len = len(os.listdir('/content/cats_dogs/data/train/dogs'))\n",
        "val_len = train_len * 0.3\n",
        "val_dogs = random.sample(os.listdir(train_dogs_path),int(val_len))\n",
        "val_cats = random.sample(os.listdir(train_cats_path),int(val_len))\n",
        "\n",
        "\n",
        "for file in val_dogs:\n",
        "  try: shutil.move(os.path.join(train_dogs_path,file), val_dogs_path)\n",
        "  except: pass\n",
        "for file in val_cats:\n",
        "  try: shutil.move(os.path.join(train_cats_path,file), val_cats_path)\n",
        "  except: pass\n",
        "  \n",
        "print(len(os.listdir(train_cats_path)))\n",
        "print(len(os.listdir(val_cats_path)))\n",
        "\n",
        "print(len(os.listdir(train_dogs_path)))\n",
        "print(len(os.listdir(val_dogs_path)))\n",
        "\n",
        "print('total train samples ', len(os.listdir(train_cats_path)) + len(os.listdir(train_dogs_path)))\n",
        "print('total train samples ', len(os.listdir(val_cats_path)) + len(os.listdir(val_dogs_path)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8750\n",
            "3750\n",
            "8750\n",
            "3750\n",
            "total train samples  17500\n",
            "total train samples  7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPtRx1P_vCBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#CNN classifier"
      ]
    },
    {
      "metadata": {
        "id": "VRXlciBPMb7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8e66820e-e4c0-4169-b9d2-9a290d849c72"
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M8bbF3egoK2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1114
        },
        "outputId": "54457061-ec3d-4368-d424-799d57ecdb07"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=32,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/32\n",
            "274/274 [==============================] - 188s 686ms/step - loss: 0.6740 - acc: 0.5869 - val_loss: 0.6211 - val_acc: 0.6712\n",
            "Epoch 2/32\n",
            "274/274 [==============================] - 179s 655ms/step - loss: 0.5808 - acc: 0.6965 - val_loss: 0.5099 - val_acc: 0.7443\n",
            "Epoch 3/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.5189 - acc: 0.7459 - val_loss: 0.4816 - val_acc: 0.7660\n",
            "Epoch 4/32\n",
            "274/274 [==============================] - 179s 655ms/step - loss: 0.4636 - acc: 0.7802 - val_loss: 0.4559 - val_acc: 0.7885\n",
            "Epoch 5/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.4238 - acc: 0.8046 - val_loss: 0.4466 - val_acc: 0.8041\n",
            "Epoch 6/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.3948 - acc: 0.8210 - val_loss: 0.3794 - val_acc: 0.8220\n",
            "Epoch 7/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.3596 - acc: 0.8415 - val_loss: 0.3292 - val_acc: 0.8616\n",
            "Epoch 8/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.3408 - acc: 0.8513 - val_loss: 0.3552 - val_acc: 0.8372\n",
            "Epoch 9/32\n",
            "274/274 [==============================] - 179s 653ms/step - loss: 0.3241 - acc: 0.8608 - val_loss: 0.3533 - val_acc: 0.8445\n",
            "Epoch 10/32\n",
            "274/274 [==============================] - 179s 652ms/step - loss: 0.3010 - acc: 0.8737 - val_loss: 0.3429 - val_acc: 0.8444\n",
            "Epoch 11/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.2818 - acc: 0.8802 - val_loss: 0.2596 - val_acc: 0.8911\n",
            "Epoch 12/32\n",
            "274/274 [==============================] - 180s 655ms/step - loss: 0.2660 - acc: 0.8868 - val_loss: 0.2561 - val_acc: 0.8924\n",
            "Epoch 13/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.2605 - acc: 0.8879 - val_loss: 0.2719 - val_acc: 0.8875\n",
            "Epoch 14/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.2396 - acc: 0.9006 - val_loss: 0.2544 - val_acc: 0.8993\n",
            "Epoch 15/32\n",
            "274/274 [==============================] - 180s 658ms/step - loss: 0.2326 - acc: 0.9043 - val_loss: 0.2647 - val_acc: 0.8960\n",
            "Epoch 16/32\n",
            "274/274 [==============================] - 181s 659ms/step - loss: 0.2189 - acc: 0.9107 - val_loss: 0.2166 - val_acc: 0.9083\n",
            "Epoch 17/32\n",
            "274/274 [==============================] - 179s 655ms/step - loss: 0.2139 - acc: 0.9140 - val_loss: 0.2131 - val_acc: 0.9143\n",
            "Epoch 18/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.2009 - acc: 0.9161 - val_loss: 0.2629 - val_acc: 0.8951\n",
            "Epoch 19/32\n",
            "274/274 [==============================] - 179s 653ms/step - loss: 0.2041 - acc: 0.9194 - val_loss: 0.4266 - val_acc: 0.8323\n",
            "Epoch 20/32\n",
            "274/274 [==============================] - 180s 655ms/step - loss: 0.1886 - acc: 0.9233 - val_loss: 0.2267 - val_acc: 0.9103\n",
            "Epoch 21/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.1995 - acc: 0.9206 - val_loss: 0.2325 - val_acc: 0.9051\n",
            "Epoch 22/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.1818 - acc: 0.9286 - val_loss: 0.2306 - val_acc: 0.9137\n",
            "Epoch 23/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.1849 - acc: 0.9283 - val_loss: 0.2134 - val_acc: 0.9184\n",
            "Epoch 24/32\n",
            "274/274 [==============================] - 179s 654ms/step - loss: 0.1790 - acc: 0.9291 - val_loss: 0.2116 - val_acc: 0.9252\n",
            "Epoch 25/32\n",
            "274/274 [==============================] - 179s 652ms/step - loss: 0.1838 - acc: 0.9294 - val_loss: 0.2344 - val_acc: 0.9049\n",
            "Epoch 26/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.1804 - acc: 0.9310 - val_loss: 0.2289 - val_acc: 0.9165\n",
            "Epoch 27/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.1714 - acc: 0.9315 - val_loss: 0.2055 - val_acc: 0.9235\n",
            "Epoch 28/32\n",
            "274/274 [==============================] - 180s 658ms/step - loss: 0.1740 - acc: 0.9317 - val_loss: 0.2156 - val_acc: 0.9120\n",
            "Epoch 29/32\n",
            "274/274 [==============================] - 180s 656ms/step - loss: 0.1733 - acc: 0.9314 - val_loss: 0.2190 - val_acc: 0.9237\n",
            "Epoch 30/32\n",
            "274/274 [==============================] - 180s 658ms/step - loss: 0.1724 - acc: 0.9307 - val_loss: 0.2098 - val_acc: 0.9240\n",
            "Epoch 31/32\n",
            "274/274 [==============================] - 180s 655ms/step - loss: 0.1706 - acc: 0.9340 - val_loss: 0.2361 - val_acc: 0.9079\n",
            "Epoch 32/32\n",
            "274/274 [==============================] - 180s 657ms/step - loss: 0.1666 - acc: 0.9358 - val_loss: 0.2564 - val_acc: 0.9175\n",
            "time taken  5755.778434276581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V9zA0ChBu75g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Transfer learning"
      ]
    },
    {
      "metadata": {
        "id": "foaln3KF94Nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "dd03be89-c235-4a66-9c3a-d8dd7e08fe2d"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'VGG16'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "vgg_model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    \n",
        "vgg_model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "vgg_model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "vgg_model.save_weights('VGG16.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "274/274 [==============================] - 326s 1s/step - loss: 0.4903 - acc: 0.7797 - val_loss: 0.3391 - val_acc: 0.8727\n",
            "Epoch 2/16\n",
            "274/274 [==============================] - 311s 1s/step - loss: 0.3377 - acc: 0.8726 - val_loss: 0.2705 - val_acc: 0.8969\n",
            "Epoch 3/16\n",
            "274/274 [==============================] - 313s 1s/step - loss: 0.2928 - acc: 0.8849 - val_loss: 0.2416 - val_acc: 0.9075\n",
            "Epoch 4/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2664 - acc: 0.8961 - val_loss: 0.2263 - val_acc: 0.9132\n",
            "Epoch 5/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.2507 - acc: 0.9002 - val_loss: 0.2202 - val_acc: 0.9147\n",
            "Epoch 6/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2404 - acc: 0.9034 - val_loss: 0.1999 - val_acc: 0.9220\n",
            "Epoch 7/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.2319 - acc: 0.9062 - val_loss: 0.1922 - val_acc: 0.9260\n",
            "Epoch 8/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2230 - acc: 0.9110 - val_loss: 0.1868 - val_acc: 0.9277\n",
            "Epoch 9/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2168 - acc: 0.9130 - val_loss: 0.1811 - val_acc: 0.9307\n",
            "Epoch 10/16\n",
            "274/274 [==============================] - 315s 1s/step - loss: 0.2119 - acc: 0.9163 - val_loss: 0.1813 - val_acc: 0.9267\n",
            "Epoch 11/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.2055 - acc: 0.9170 - val_loss: 0.1747 - val_acc: 0.9315\n",
            "Epoch 12/16\n",
            "274/274 [==============================] - 312s 1s/step - loss: 0.2009 - acc: 0.9176 - val_loss: 0.1700 - val_acc: 0.9321\n",
            "Epoch 13/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.1965 - acc: 0.9216 - val_loss: 0.1736 - val_acc: 0.9304\n",
            "Epoch 14/16\n",
            "274/274 [==============================] - 314s 1s/step - loss: 0.1942 - acc: 0.9214 - val_loss: 0.1692 - val_acc: 0.9324\n",
            "Epoch 15/16\n",
            "274/274 [==============================] - 313s 1s/step - loss: 0.1901 - acc: 0.9235 - val_loss: 0.1628 - val_acc: 0.9341\n",
            "Epoch 16/16\n",
            "274/274 [==============================] - 313s 1s/step - loss: 0.1866 - acc: 0.9267 - val_loss: 0.1607 - val_acc: 0.9364\n",
            "time taken  5039.052234888077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e5tJZ2Y6vg3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IlpiMhvMd69-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "a7682366-8ddd-4f5b-d300-ef7561639bc4"
      },
      "cell_type": "code",
      "source": [
        "BASE_MODEL = 'ResNet50'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator,\n",
        "        #class_weight = class_weights,\n",
        "        callbacks=[ModelCheckpoint('ResNet50-transferlearning.model', monitor='val_acc', save_best_only=True)])\n",
        "model.save_weights('ResNet50.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Epoch 1/16\n",
            "547/547 [==============================] - 320s 585ms/step - loss: 0.3268 - acc: 0.8855 - val_loss: 0.1813 - val_acc: 0.9453\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 317s 580ms/step - loss: 0.1606 - acc: 0.9486 - val_loss: 0.1292 - val_acc: 0.9579\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 316s 577ms/step - loss: 0.1262 - acc: 0.9563 - val_loss: 0.1124 - val_acc: 0.9603\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 323s 590ms/step - loss: 0.1092 - acc: 0.9616 - val_loss: 0.0993 - val_acc: 0.9648\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 316s 577ms/step - loss: 0.1042 - acc: 0.9617 - val_loss: 0.0907 - val_acc: 0.9653\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 321s 587ms/step - loss: 0.0989 - acc: 0.9637 - val_loss: 0.0899 - val_acc: 0.9669\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 317s 579ms/step - loss: 0.0946 - acc: 0.9646 - val_loss: 0.0845 - val_acc: 0.9685\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 323s 591ms/step - loss: 0.0875 - acc: 0.9690 - val_loss: 0.0872 - val_acc: 0.9667\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 321s 586ms/step - loss: 0.0924 - acc: 0.9657 - val_loss: 0.0807 - val_acc: 0.9693\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 325s 594ms/step - loss: 0.0845 - acc: 0.9692 - val_loss: 0.0816 - val_acc: 0.9680\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 319s 584ms/step - loss: 0.0834 - acc: 0.9678 - val_loss: 0.0804 - val_acc: 0.9685\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 325s 593ms/step - loss: 0.0832 - acc: 0.9681 - val_loss: 0.0756 - val_acc: 0.9697\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 319s 583ms/step - loss: 0.0791 - acc: 0.9711 - val_loss: 0.0772 - val_acc: 0.9689\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 326s 597ms/step - loss: 0.0803 - acc: 0.9695 - val_loss: 0.0722 - val_acc: 0.9721\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 318s 581ms/step - loss: 0.0770 - acc: 0.9705 - val_loss: 0.0764 - val_acc: 0.9699\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 323s 591ms/step - loss: 0.0766 - acc: 0.9715 - val_loss: 0.0727 - val_acc: 0.9709\n",
            "time taken  5178.096745729446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sb5kUTPTd7BJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QiBcM20Cd67Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}